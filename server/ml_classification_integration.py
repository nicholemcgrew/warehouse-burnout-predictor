# -*- coding: utf-8 -*-
"""ml-classification-integration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dehXM9ZIoV-x4iN43PY4wpcEpHXwMNiw
"""

#!/usr/bin/env python3
"""
Fixed Unified YouTube + Glassdoor ML Integration

This script fixes the feature alignment issues and creates a robust integration
that works with both YouTube transcript data and Glassdoor data.
"""

import pandas as pd
import numpy as np
import re
from textblob import TextBlob
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

class FixedUnifiedProcessor:
    """
    Fixed unified processor that ensures proper feature alignment
    """

    def __init__(self):
        self.model = None
        # EXACT feature columns matching your original Colab model
        self.feature_columns = [
            'rating_overall', 'rating_work_life', 'rating_culture_values',
            'rating_senior_leadership', 'rating_compensation_benefits',
            'is_regular', 'is_part_time', 'is_contract', 'is_current', 'is_former',
            'employee_length', 'cons_length', 'pros_length', 'advice_length',
            'burnout_keywords', 'physical_keywords', 'management_keywords',
            'workload_keywords', 'environment_keywords',
            'location_canada', 'location_california'
        ]

        # Enhanced keyword dictionaries
        self.unified_keywords = {
            'burnout_keywords': [
                'burnout', 'burnt out', 'burned out', 'exhausted', 'drained',
                'stress', 'stressed', 'stressful', 'overwhelmed', 'overwhelming',
                'can\'t cope', 'too much', 'mentally tired', 'breakdown'
            ],
            'physical_keywords': [
                'physically demanding', 'heavy lifting', 'sore', 'pain', 'tired',
                'back pain', 'feet hurt', 'leg pain', 'shoulder pain', 'headache',
                'fatigue', 'aching', 'hurt', 'physical toll', 'standing', 'walking'
            ],
            'management_keywords': [
                'micromanage', 'bad manager', 'poor management', 'unfair', 'biased',
                'management', 'supervisor', 'boss', 'leadership', 'unreasonable',
                'no support', 'unsupportive', 'favoritism', 'discrimination'
            ],
            'workload_keywords': [
                'overwhelmed', 'too much work', 'pressure', 'quota', 'unrealistic',
                'rushed', 'workload', 'deadlines', 'targets', 'metrics',
                'overtime', 'long hours', 'busy', 'hectic', 'peak season'
            ],
            'environment_keywords': [
                'toxic', 'hostile', 'negative', 'culture', 'politics',
                'negative environment', 'bad atmosphere', 'drama'
            ]
        }

    def create_feature_vector(self, **kwargs):
        """
        Create a properly formatted feature vector with all required features
        """
        # Initialize all features with defaults
        features = {
            'rating_overall': kwargs.get('rating_overall', 3),
            'rating_work_life': kwargs.get('rating_work_life', 3),
            'rating_culture_values': kwargs.get('rating_culture_values', 3),
            'rating_senior_leadership': kwargs.get('rating_senior_leadership', 3),
            'rating_compensation_benefits': kwargs.get('rating_compensation_benefits', 3),
            'is_regular': kwargs.get('is_regular', 1),
            'is_part_time': kwargs.get('is_part_time', 0),
            'is_contract': kwargs.get('is_contract', 0),
            'is_current': kwargs.get('is_current', 1),
            'is_former': kwargs.get('is_former', 0),
            'employee_length': kwargs.get('employee_length', 12),
            'cons_length': kwargs.get('cons_length', 100),
            'pros_length': kwargs.get('pros_length', 50),
            'advice_length': kwargs.get('advice_length', 25),
            'burnout_keywords': kwargs.get('burnout_keywords', 0),
            'physical_keywords': kwargs.get('physical_keywords', 0),
            'management_keywords': kwargs.get('management_keywords', 0),
            'workload_keywords': kwargs.get('workload_keywords', 0),
            'environment_keywords': kwargs.get('environment_keywords', 0),
            'location_canada': kwargs.get('location_canada', 0),
            'location_california': kwargs.get('location_california', 0)
        }

        return pd.DataFrame([features])

    def analyze_text_for_keywords(self, text):
        """
        Analyze text and return keyword counts
        """
        if pd.isna(text) or not text:
            return {key: 0 for key in self.unified_keywords.keys()}

        text_lower = str(text).lower()
        keyword_counts = {}

        for category, keywords in self.unified_keywords.items():
            count = sum(1 for keyword in keywords if keyword in text_lower)
            keyword_counts[category] = count

        return keyword_counts

    def process_youtube_data(self, youtube_df):
        """
        Process YouTube data to ML features with proper alignment
        """
        print("ðŸŽ¥ PROCESSING YOUTUBE DATA")
        print("=" * 40)

        # Filter warehouse-related content
        if 'is_warehouse' in youtube_df.columns:
            youtube_df = youtube_df[youtube_df['is_warehouse'] == True].copy()
            print(f"ðŸ“¦ Using {len(youtube_df)} warehouse-related records")

        processed_features = []

        for idx, row in youtube_df.iterrows():
            # Extract basic features
            text = row.get('text', '')
            sentiment_polarity = row.get('sentiment_polarity', 0)
            rating_overall = row.get('rating_overall', 3)

            # Convert sentiment to rating if needed
            if pd.isna(rating_overall) or rating_overall == 0:
                rating_overall = max(1, min(5, (sentiment_polarity + 1) * 2.5))

            # Analyze work-life balance from text
            wl_negative = ['overtime', 'long hours', 'exhausted', 'tired', 'burnout']
            wl_score = max(1, 5 - sum(1 for term in wl_negative if term in text.lower()))

            # Analyze keywords
            keyword_counts = self.analyze_text_for_keywords(text)

            # Create feature vector
            features = self.create_feature_vector(
                rating_overall=rating_overall,
                rating_work_life=wl_score,
                rating_culture_values=rating_overall,
                rating_senior_leadership=rating_overall,
                rating_compensation_benefits=rating_overall,
                is_current=1 if row.get('employee_status') == 'employed' else 0,
                is_former=1 if row.get('employee_status') != 'employed' else 0,
                cons_length=len(text),
                **keyword_counts
            )

            # Determine stress level
            stress_score = (5 - rating_overall) + sum(keyword_counts.values())
            if stress_score >= 5:
                stress_level = 'High Stress'
            elif stress_score >= 2:
                stress_level = 'Medium Stress'
            else:
                stress_level = 'Low Stress'

            features['stress_level'] = stress_level
            features['data_source'] = 'youtube'
            features['original_text'] = text

            processed_features.append(features)

        # Combine all features
        result_df = pd.concat(processed_features, ignore_index=True)

        print(f"âœ… Processed {len(result_df)} YouTube records")
        print("Stress distribution:")
        print(result_df['stress_level'].value_counts())

        return result_df

    def process_glassdoor_data(self, glassdoor_df):
        """
        Process Glassdoor data to ML features with proper alignment
        """
        print("ðŸ¢ PROCESSING GLASSDOOR DATA")
        print("=" * 40)

        processed_features = []

        for idx, row in glassdoor_df.iterrows():
            # Extract ratings
            rating_overall = row.get('rating_overall', 3)
            rating_work_life = row.get('rating_work_life', 3)

            # Get text
            text = row.get('cleaned_text', row.get('review_text', ''))

            # Analyze keywords
            keyword_counts = self.analyze_text_for_keywords(text)

            # Map glassdoor-specific features if available
            if 'work_life_balance_mentions' in row:
                keyword_counts['burnout_keywords'] += row.get('work_life_balance_mentions', 0)
            if 'physical_demands_mentions' in row:
                keyword_counts['physical_keywords'] += row.get('physical_demands_mentions', 0)
            if 'management_issues_mentions' in row:
                keyword_counts['management_keywords'] += row.get('management_issues_mentions', 0)
            if 'workload_stress_mentions' in row:
                keyword_counts['workload_keywords'] += row.get('workload_stress_mentions', 0)
            if 'workplace_culture_mentions' in row:
                keyword_counts['environment_keywords'] += row.get('workplace_culture_mentions', 0)

            # Create feature vector
            features = self.create_feature_vector(
                rating_overall=rating_overall,
                rating_work_life=rating_work_life,
                rating_culture_values=row.get('rating_culture_values', rating_overall),
                rating_senior_leadership=row.get('rating_senior_leadership', rating_overall),
                rating_compensation_benefits=row.get('rating_compensation_benefits', rating_overall),
                cons_length=len(str(text)),
                **keyword_counts
            )

            # Determine stress level
            if 'burnout_score_normalized' in row:
                score = row['burnout_score_normalized']
                if score >= 60:
                    stress_level = 'High Stress'
                elif score >= 30:
                    stress_level = 'Medium Stress'
                else:
                    stress_level = 'Low Stress'
            else:
                # Calculate from ratings and keywords
                stress_score = (5 - rating_overall) + sum(keyword_counts.values())
                if stress_score >= 5:
                    stress_level = 'High Stress'
                elif stress_score >= 2:
                    stress_level = 'Medium Stress'
                else:
                    stress_level = 'Low Stress'

            features['stress_level'] = stress_level
            features['data_source'] = 'glassdoor'
            features['original_text'] = text

            processed_features.append(features)

        # Combine all features
        result_df = pd.concat(processed_features, ignore_index=True)

        print(f"âœ… Processed {len(result_df)} Glassdoor records")
        print("Stress distribution:")
        print(result_df['stress_level'].value_counts())

        return result_df

    def train_model(self, combined_df):
        """
        Train model ensuring proper feature alignment
        """
        print("\nðŸ¤– TRAINING MODEL WITH FIXED FEATURES")
        print("=" * 50)

        # Ensure we have the exact features in the exact order
        X = combined_df[self.feature_columns].fillna(0)
        y = combined_df['stress_level']

        # Verify feature alignment
        print(f"Feature columns: {self.feature_columns}")
        print(f"Data columns: {list(X.columns)}")
        print(f"Features match: {list(X.columns) == self.feature_columns}")

        # Remove invalid rows
        valid_mask = y.isin(['High Stress', 'Medium Stress', 'Low Stress'])
        X = X[valid_mask]
        y = y[valid_mask]

        print(f"Training data: {len(X)} records")
        print(f"Target distribution:")
        print(y.value_counts())

        # Split data
        if len(X) < 10:
            print("âš ï¸ Small dataset - using simple train/test split")
            test_size = 0.2
        else:
            test_size = 0.3

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42,
            stratify=y if len(y.value_counts()) > 1 else None
        )

        # Train model
        self.model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            class_weight='balanced'
        )

        self.model.fit(X_train, y_train)

        # Evaluate
        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)

        print(f"\nðŸŽ¯ MODEL PERFORMANCE:")
        print(f"Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)")

        if len(X_test) > 0:
            print(f"\nðŸ“‹ Classification Report:")
            print(classification_report(y_test, y_pred))

        # Feature importance
        feature_importance = pd.DataFrame({
            'feature': self.feature_columns,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)

        print(f"\nðŸ”‘ TOP 10 MOST IMPORTANT FEATURES:")
        print(feature_importance.head(10))

        return accuracy, feature_importance

    def create_prediction_function(self):
        """
        Create prediction function with proper feature alignment
        """
        def predict_stress(data):
            """
            Predict stress level ensuring proper feature alignment
            """
            if self.model is None:
                raise ValueError("Model not trained yet!")

            # Handle different input formats
            if isinstance(data, dict):
                # API format input
                review_text = data.get("review", "")
                overall_rating = data.get("overallRating", 3)
                work_life_balance = data.get("workLifeBalance", 3)
                culture = data.get("culture", 3)
                employee_status = data.get("status", "REGULAR")

                # Analyze keywords from text
                keyword_counts = self.analyze_text_for_keywords(review_text)

                # Create feature vector
                features = self.create_feature_vector(
                    rating_overall=overall_rating,
                    rating_work_life=work_life_balance,
                    rating_culture_values=culture,
                    rating_senior_leadership=culture,
                    rating_compensation_benefits=culture,
                    is_regular=1 if employee_status == 'REGULAR' else 0,
                    is_part_time=1 if employee_status == 'PART_TIME' else 0,
                    is_contract=1 if employee_status == 'CONTRACT' else 0,
                    cons_length=len(review_text),
                    **keyword_counts
                )

            elif isinstance(data, pd.DataFrame):
                # DataFrame input - ensure proper column order
                features = data[self.feature_columns].fillna(0)
            else:
                raise ValueError("Data must be dict or DataFrame")

            # Make prediction
            prediction = self.model.predict(features)[0]
            probabilities = self.model.predict_proba(features)[0]

            # Create result
            classes = self.model.classes_
            prob_dict = dict(zip(classes, probabilities))
            confidence = max(probabilities)

            # Apply enhanced detection rules for API input
            if isinstance(data, dict):
                review_text = data.get("review", "").lower()
                stress_boost = 0

                # Boost rules
                if any(word in review_text for word in ['hate', 'awful', 'terrible', 'worst']):
                    stress_boost += 2
                if work_life_balance <= 2:
                    stress_boost += 1
                if overall_rating <= 2:
                    stress_boost += 1

                # Apply boost
                if stress_boost >= 3:
                    prediction = 'High Stress'
                    confidence = min(0.85 + stress_boost * 0.03, 0.95)
                elif stress_boost >= 1.5 and prediction == 'Low Stress':
                    prediction = 'Medium Stress'
                    confidence = 0.75

            return {
                "prediction": prediction,
                "confidence": round(confidence, 2),
                "probabilities": {k: round(v, 2) for k, v in prob_dict.items()},
                "recommendation": {
                    "High Stress": "IMMEDIATE INTERVENTION REQUIRED",
                    "Medium Stress": "MONITOR CLOSELY & PROVIDE SUPPORT",
                    "Low Stress": "LOW RISK â€“ MAINTAIN SUPPORTIVE ENVIRONMENT"
                }[prediction],
                "status": {
                    "High Stress": "RED ALERT",
                    "Medium Stress": "YELLOW WARNING",
                    "Low Stress": "GREEN STATUS"
                }[prediction]
            }

        return predict_stress

def run_fixed_integration(youtube_file=None, glassdoor_file=None):
    """
    Run the fixed integration with proper feature alignment
    """
    print("ðŸš€ FIXED UNIFIED INTEGRATION")
    print("=" * 50)

    processor = FixedUnifiedProcessor()

    # Load and process datasets
    datasets = []

    if youtube_file:
        try:
            youtube_df = pd.read_csv(youtube_file)
            youtube_features = processor.process_youtube_data(youtube_df)
            datasets.append(youtube_features)
        except Exception as e:
            print(f"âŒ Error processing YouTube data: {e}")

    if glassdoor_file:
        try:
            glassdoor_df = pd.read_csv(glassdoor_file)
            glassdoor_features = processor.process_glassdoor_data(glassdoor_df)
            datasets.append(glassdoor_features)
        except Exception as e:
            print(f"âŒ Error processing Glassdoor data: {e}")

    if not datasets:
        print("âŒ No datasets successfully processed")
        return None, None

    # Combine datasets
    combined_df = pd.concat(datasets, ignore_index=True)

    print(f"\nðŸ“Š COMBINED DATASET:")
    print(f"Total records: {len(combined_df)}")
    if 'data_source' in combined_df.columns:
        print("Data source distribution:")
        print(combined_df['data_source'].value_counts())
    print("Stress level distribution:")
    print(combined_df['stress_level'].value_counts())

    # Train model
    accuracy, feature_importance = processor.train_model(combined_df)

    # Create prediction function
    predict_func = processor.create_prediction_function()

    print(f"\nâœ… INTEGRATION COMPLETE!")
    print(f"   â€¢ Model trained successfully")
    print(f"   â€¢ Accuracy: {accuracy:.1%}")
    print(f"   â€¢ Ready for predictions!")

    # Test predictions
    print(f"\nðŸ§ª TESTING PREDICTIONS:")

    test_cases = [
        {
            "name": "High Stress Employee",
            "data": {
                "overallRating": 1,
                "workLifeBalance": 1,
                "culture": 2,
                "status": "REGULAR",
                "review": "I hate this job, it's exhausting and management is terrible"
            }
        },
        {
            "name": "Medium Stress Employee",
            "data": {
                "overallRating": 3,
                "workLifeBalance": 2,
                "culture": 3,
                "status": "REGULAR",
                "review": "The work is demanding and sometimes overwhelming"
            }
        },
        {
            "name": "Low Stress Employee",
            "data": {
                "overallRating": 5,
                "workLifeBalance": 4,
                "culture": 5,
                "status": "REGULAR",
                "review": "Great place to work with good benefits"
            }
        }
    ]

    for test_case in test_cases:
        try:
            result = predict_func(test_case["data"])
            print(f"\n{test_case['name']}:")
            print(f"   Prediction: {result['prediction']}")
            print(f"   Confidence: {result['confidence']}")
            print(f"   Status: {result['status']}")
        except Exception as e:
            print(f"\nâŒ Error testing {test_case['name']}: {e}")

    return processor, predict_func

# Simple standalone function for immediate use
def create_simple_youtube_model(youtube_file):
    """
    Create a simple model from just YouTube data
    """
    print("ðŸŽ¬ CREATING SIMPLE YOUTUBE MODEL")
    print("=" * 40)

    # Load data
    df = pd.read_csv(youtube_file)
    if 'is_warehouse' in df.columns:
        df = df[df['is_warehouse'] == True]

    # Create simple features
    features = []
    for _, row in df.iterrows():
        text = str(row.get('text', '')).lower()
        sentiment = row.get('sentiment_polarity', 0)
        rating = row.get('rating_overall', 3)

        # Simple keyword counts
        stress_words = ['stress', 'exhausted', 'overwhelmed', 'burnout', 'tired']
        physical_words = ['pain', 'sore', 'hurt', 'aching', 'demanding']
        negative_words = ['hate', 'terrible', 'awful', 'worst', 'horrible']

        stress_count = sum(1 for word in stress_words if word in text)
        physical_count = sum(1 for word in physical_words if word in text)
        negative_count = sum(1 for word in negative_words if word in text)

        # Determine stress level
        total_negative = stress_count + physical_count + negative_count
        if total_negative >= 2 or rating <= 2:
            stress_level = 'High Stress'
        elif total_negative >= 1 or rating <= 3:
            stress_level = 'Medium Stress'
        else:
            stress_level = 'Low Stress'

        features.append({
            'rating': rating,
            'sentiment': sentiment,
            'text_length': len(text),
            'stress_keywords': stress_count,
            'physical_keywords': physical_count,
            'negative_keywords': negative_count,
            'stress_level': stress_level
        })

    feature_df = pd.DataFrame(features)

    print(f"Created {len(feature_df)} feature records")
    print("Stress distribution:")
    print(feature_df['stress_level'].value_counts())

    # Simple prediction function
    def simple_predict(review_text, rating=3):
        text_lower = review_text.lower()

        stress_words = ['stress', 'exhausted', 'overwhelmed', 'burnout', 'tired']
        negative_words = ['hate', 'terrible', 'awful', 'worst', 'horrible']

        stress_count = sum(1 for word in stress_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)

        total_negative = stress_count + negative_count

        if total_negative >= 2 or rating <= 2:
            return "High Stress"
        elif total_negative >= 1 or rating <= 3:
            return "Medium Stress"
        else:
            return "Low Stress"

    return simple_predict, feature_df

# Example usage
if __name__ == "__main__":
    # Try the fixed integration
    processor, predict_func = run_fixed_integration(
        youtube_file='youtube_transcript_dataset.csv'
    )

    if processor is None:
        print("\nðŸ”„ FALLING BACK TO SIMPLE MODEL")
        simple_predict, simple_df = create_simple_youtube_model('youtube_transcript_dataset.csv')

        print("\nðŸ§ª Testing simple model:")
        test_text = "I hate this job, it's exhausting and overwhelming"
        result = simple_predict(test_text, rating=2)
        print(f"'{test_text}' -> {result}")

#!/usr/bin/env python3
"""
Feature Importance Visualization for YouTube + Glassdoor ML Model

This code creates beautiful horizontal bar charts to visualize feature importance
from your stress prediction model.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

def create_sample_feature_importance():
    """
    Create sample feature importance data (replace with your actual model's feature_importances_)
    """
    feature_names = [
        'rating_overall', 'rating_work_life', 'rating_culture_values',
        'rating_senior_leadership', 'rating_compensation_benefits',
        'is_regular', 'is_part_time', 'is_contract', 'is_current', 'is_former',
        'employee_length', 'cons_length', 'pros_length', 'advice_length',
        'burnout_keywords', 'physical_keywords', 'management_keywords',
        'workload_keywords', 'environment_keywords',
        'location_canada', 'location_california'
    ]

    # Sample importance values (you'll replace this with model.feature_importances_)
    importance_values = [
        0.173, 0.155, 0.153, 0.145, 0.119, 0.107, 0.063, 0.029, 0.019, 0.011,
        0.008, 0.007, 0.006, 0.005, 0.004, 0.003, 0.003, 0.002, 0.002, 0.001, 0.001
    ]

    return pd.DataFrame({
        'feature': feature_names,
        'importance': importance_values
    }).sort_values('importance', ascending=False)

def plot_feature_importance_horizontal(feature_importance_df, top_n=10, figsize=(12, 8)):
    """
    Create a beautiful horizontal bar chart for feature importance

    Parameters:
    - feature_importance_df: DataFrame with 'feature' and 'importance' columns
    - top_n: Number of top features to display
    - figsize: Figure size (width, height)
    """

    # Select top N features
    top_features = feature_importance_df.head(top_n)

    # Create the plot
    plt.figure(figsize=figsize)

    # Set style
    plt.style.use('default')
    sns.set_palette("husl")

    # Create horizontal bar plot
    bars = plt.barh(range(len(top_features)), top_features['importance'],
                    color=plt.cm.viridis(np.linspace(0, 1, len(top_features))))

    # Customize the plot
    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('Feature Importance', fontsize=12, fontweight='bold')
    plt.ylabel('Features', fontsize=12, fontweight='bold')
    plt.title(f'Top {top_n} Most Important Features\nAmazon Employee Stress Prediction Model',
              fontsize=14, fontweight='bold', pad=20)

    # Add value labels on bars
    for i, (bar, importance) in enumerate(zip(bars, top_features['importance'])):
        plt.text(importance + 0.002, i, f'{importance:.3f}',
                va='center', ha='left', fontweight='bold', fontsize=10)

    # Customize grid
    plt.grid(axis='x', alpha=0.3, linestyle='--')

    # Invert y-axis to show highest importance at top
    plt.gca().invert_yaxis()

    # Adjust layout
    plt.tight_layout()

    # Add background color
    plt.gca().set_facecolor('#f8f9fa')

    plt.show()

def plot_feature_importance_grouped(feature_importance_df, figsize=(14, 10)):
    """
    Create a grouped horizontal bar chart by feature categories
    """

    # Define feature categories
    feature_categories = {
        'Rating Features': ['rating_overall', 'rating_work_life', 'rating_culture_values',
                           'rating_senior_leadership', 'rating_compensation_benefits'],
        'Employee Status': ['is_regular', 'is_part_time', 'is_contract', 'is_current',
                           'is_former', 'employee_length'],
        'Text Features': ['cons_length', 'pros_length', 'advice_length'],
        'Keyword Features': ['burnout_keywords', 'physical_keywords', 'management_keywords',
                            'workload_keywords', 'environment_keywords'],
        'Location Features': ['location_canada', 'location_california']
    }

    # Create categorized data
    categorized_data = []
    for category, features in feature_categories.items():
        for feature in features:
            importance_row = feature_importance_df[feature_importance_df['feature'] == feature]
            if not importance_row.empty:
                categorized_data.append({
                    'category': category,
                    'feature': feature,
                    'importance': importance_row['importance'].iloc[0]
                })

    cat_df = pd.DataFrame(categorized_data)

    # Create the plot
    fig, ax = plt.subplots(figsize=figsize)

    # Color palette for categories
    colors = plt.cm.Set3(np.linspace(0, 1, len(feature_categories)))
    category_colors = dict(zip(feature_categories.keys(), colors))

    # Plot bars
    y_pos = 0
    y_labels = []
    y_positions = []

    for category in feature_categories.keys():
        cat_data = cat_df[cat_df['category'] == category].sort_values('importance', ascending=False)

        if not cat_data.empty:
            # Add category separator
            if y_pos > 0:
                y_pos += 0.5  # Add spacing between categories

            # Plot bars for this category
            for _, row in cat_data.iterrows():
                bars = ax.barh(y_pos, row['importance'], color=category_colors[category],
                              alpha=0.8, height=0.8)

                # Add value label
                ax.text(row['importance'] + 0.002, y_pos, f'{row["importance"]:.3f}',
                       va='center', ha='left', fontweight='bold', fontsize=9)

                y_labels.append(row['feature'])
                y_positions.append(y_pos)
                y_pos += 1

    # Customize the plot
    ax.set_yticks(y_positions)
    ax.set_yticklabels(y_labels)
    ax.set_xlabel('Feature Importance', fontsize=12, fontweight='bold')
    ax.set_ylabel('Features (Grouped by Category)', fontsize=12, fontweight='bold')
    ax.set_title('Feature Importance by Category\nAmazon Employee Stress Prediction Model',
                fontsize=14, fontweight='bold', pad=20)

    # Add grid
    ax.grid(axis='x', alpha=0.3, linestyle='--')
    ax.set_facecolor('#f8f9fa')

    # Invert y-axis
    ax.invert_yaxis()

    # Add legend
    legend_elements = [plt.Rectangle((0,0),1,1, facecolor=category_colors[cat], alpha=0.8, label=cat)
                      for cat in feature_categories.keys()]
    ax.legend(handles=legend_elements, loc='lower right', framealpha=0.9)

    plt.tight_layout()
    plt.show()

def plot_feature_importance_comparison(model1_importance, model2_importance=None,
                                     model1_label="YouTube Model", model2_label="Combined Model",
                                     top_n=10, figsize=(14, 8)):
    """
    Compare feature importance between two models
    """

    if model2_importance is None:
        # Create sample comparison data
        model2_importance = model1_importance.copy()
        model2_importance['importance'] = model2_importance['importance'] * np.random.uniform(0.8, 1.2, len(model2_importance))

    # Get top features from both models
    top_features = list(set(model1_importance.head(top_n)['feature'].tolist() +
                           model2_importance.head(top_n)['feature'].tolist()))[:top_n]

    # Prepare data
    comparison_data = []
    for feature in top_features:
        imp1 = model1_importance[model1_importance['feature'] == feature]['importance']
        imp2 = model2_importance[model2_importance['feature'] == feature]['importance']

        comparison_data.append({
            'feature': feature,
            model1_label: imp1.iloc[0] if not imp1.empty else 0,
            model2_label: imp2.iloc[0] if not imp2.empty else 0
        })

    comp_df = pd.DataFrame(comparison_data)
    comp_df = comp_df.sort_values(model1_label, ascending=False)

    # Create the plot
    fig, ax = plt.subplots(figsize=figsize)

    x = np.arange(len(comp_df))
    width = 0.35

    # Create bars
    bars1 = ax.barh(x - width/2, comp_df[model1_label], width,
                    label=model1_label, alpha=0.8, color='#2E86AB')
    bars2 = ax.barh(x + width/2, comp_df[model2_label], width,
                    label=model2_label, alpha=0.8, color='#A23B72')

    # Customize
    ax.set_yticks(x)
    ax.set_yticklabels(comp_df['feature'])
    ax.set_xlabel('Feature Importance', fontsize=12, fontweight='bold')
    ax.set_ylabel('Features', fontsize=12, fontweight='bold')
    ax.set_title('Feature Importance Comparison\nYouTube vs Combined Model',
                fontsize=14, fontweight='bold', pad=20)

    # Add value labels
    for bars in [bars1, bars2]:
        for bar in bars:
            width_val = bar.get_width()
            if width_val > 0.01:  # Only show labels for significant values
                ax.text(width_val + 0.002, bar.get_y() + bar.get_height()/2,
                       f'{width_val:.3f}', va='center', ha='left', fontsize=9)

    ax.legend(loc='lower right')
    ax.grid(axis='x', alpha=0.3, linestyle='--')
    ax.invert_yaxis()
    ax.set_facecolor('#f8f9fa')

    plt.tight_layout()
    plt.show()

def create_interactive_feature_plot(feature_importance_df, top_n=10):
    """
    Create an interactive feature importance plot (requires plotly)
    """
    try:
        import plotly.graph_objects as go
        import plotly.express as px

        top_features = feature_importance_df.head(top_n)

        fig = go.Figure(go.Bar(
            x=top_features['importance'],
            y=top_features['feature'],
            orientation='h',
            marker=dict(
                color=top_features['importance'],
                colorscale='Viridis',
                showscale=True,
                colorbar=dict(title="Importance")
            ),
            text=[f'{imp:.3f}' for imp in top_features['importance']],
            textposition='outside',
            textfont=dict(size=12)
        ))

        fig.update_layout(
            title={
                'text': f'Top {top_n} Most Important Features<br>Amazon Employee Stress Prediction Model',
                'x': 0.5,
                'xanchor': 'center',
                'font': {'size': 16}
            },
            xaxis_title='Feature Importance',
            yaxis_title='Features',
            height=600,
            margin=dict(l=200, r=100, t=100, b=50),
            yaxis=dict(autorange='reversed'),
            template='plotly_white'
        )

        fig.show()

    except ImportError:
        print("âŒ Plotly not installed. Install with: pip install plotly")
        print("ðŸ”„ Falling back to matplotlib...")
        plot_feature_importance_horizontal(feature_importance_df, top_n)

# Example usage functions

def visualize_youtube_model_features():
    """
    Example: Visualize features from YouTube model
    """
    print("ðŸ“Š VISUALIZING YOUTUBE MODEL FEATURES")
    print("=" * 50)

    # Load your YouTube data and train model
    # (Replace this with your actual model training code)

    # For demonstration, create sample feature importance
    feature_importance = create_sample_feature_importance()

    print("Creating visualizations...")

    # 1. Standard horizontal bar chart
    print("\n1. Standard Feature Importance Chart:")
    plot_feature_importance_horizontal(feature_importance, top_n=15)

    # 2. Grouped by categories
    print("\n2. Grouped by Feature Categories:")
    plot_feature_importance_grouped(feature_importance)

    # 3. Interactive plot (if plotly available)
    print("\n3. Interactive Feature Importance:")
    create_interactive_feature_plot(feature_importance, top_n=12)

    return feature_importance

def extract_feature_importance_from_model(model, feature_columns):
    """
    Extract feature importance from a trained sklearn model

    Parameters:
    - model: Trained sklearn model (RandomForestClassifier, etc.)
    - feature_columns: List of feature column names

    Returns:
    - DataFrame with feature importance
    """

    if not hasattr(model, 'feature_importances_'):
        raise ValueError("Model doesn't have feature_importances_ attribute")

    importance_df = pd.DataFrame({
        'feature': feature_columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)

    return importance_df

def save_feature_importance_plots(feature_importance_df, save_path="feature_importance"):
    """
    Save all feature importance plots to files
    """
    import matplotlib
    matplotlib.use('Agg')  # Use non-interactive backend

    # Save standard plot
    plt.figure(figsize=(12, 8))
    plot_feature_importance_horizontal(feature_importance_df, top_n=15)
    plt.savefig(f'{save_path}_standard.png', dpi=300, bbox_inches='tight')
    plt.close()

    # Save grouped plot
    plt.figure(figsize=(14, 10))
    plot_feature_importance_grouped(feature_importance_df)
    plt.savefig(f'{save_path}_grouped.png', dpi=300, bbox_inches='tight')
    plt.close()

    print(f"âœ… Plots saved as {save_path}_standard.png and {save_path}_grouped.png")

# Quick integration with your existing code

def integrate_with_your_model(your_trained_model, your_feature_columns):
    """
    Quick integration function to use with your existing model

    Usage:
    # After training your model:
    feature_importance_df = integrate_with_your_model(model, feature_columns)
    """

    # Extract feature importance
    feature_importance_df = extract_feature_importance_from_model(
        your_trained_model, your_feature_columns
    )

    print("ðŸŽ¯ MODEL FEATURE IMPORTANCE ANALYSIS")
    print("=" * 50)
    print(f"Total features: {len(feature_importance_df)}")
    print(f"\nTop 10 features:")
    print(feature_importance_df.head(10))

    # Create visualizations
    print("\nðŸ“Š Creating visualizations...")

    # Standard plot
    plot_feature_importance_horizontal(feature_importance_df, top_n=15)

    # Grouped plot
    plot_feature_importance_grouped(feature_importance_df)

    return feature_importance_df

# Example of how to use with your actual trained model:

"""
INTEGRATION EXAMPLE:

# After training your model (from your existing code):
from sklearn.ensemble import RandomForestClassifier

# Your model training code here...
model = RandomForestClassifier(...)
model.fit(X_train, y_train)

# Extract and visualize feature importance:
feature_importance_df = integrate_with_your_model(model, feature_columns)

# Or use individual functions:
feature_importance_df = extract_feature_importance_from_model(model, feature_columns)
plot_feature_importance_horizontal(feature_importance_df, top_n=20)
plot_feature_importance_grouped(feature_importance_df)

# Save plots:
save_feature_importance_plots(feature_importance_df, "my_model_features")
"""

if __name__ == "__main__":
    # Run example visualization
    feature_importance = visualize_youtube_model_features()

    print("\nâœ… Feature importance visualization complete!")
    print("\nðŸ’¡ TO USE WITH YOUR MODEL:")
    print("   feature_importance_df = integrate_with_your_model(your_model, your_feature_columns)")

#!/usr/bin/env python3
"""
Simple Burnout Relationship Visualizations
Using only matplotlib and basic pandas - easy to integrate with your existing code
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

def create_sample_data():
    """Create sample data - replace with your actual dataset"""
    np.random.seed(42)
    n = 200

    # Create realistic relationships
    data = {
        'rating_work_life': np.random.choice([1,2,3,4,5], n, p=[0.2,0.3,0.3,0.15,0.05]),
        'rating_overall': np.random.choice([1,2,3,4,5], n, p=[0.15,0.25,0.35,0.2,0.05]),
        'burnout_keywords': np.random.poisson(2, n),
        'physical_keywords': np.random.poisson(1.5, n),
        'employee_length_months': np.random.choice([1,3,6,12,24,36], n),
        'review_length': np.random.normal(300, 150, n)
    }

    df = pd.DataFrame(data)
    df['review_length'] = np.maximum(df['review_length'], 50)  # Minimum length

    # Calculate burnout risk based on features
    df['burnout_score'] = (
        (5 - df['rating_work_life']) * 20 +
        (5 - df['rating_overall']) * 15 +
        df['burnout_keywords'] * 10 +
        df['physical_keywords'] * 8
    )

    # Create binary burnout indicator
    df['high_burnout'] = (df['burnout_score'] > df['burnout_score'].quantile(0.6)).astype(int)

    return df

def visual_1_ratings_burnout(df):
    """Visual 1: Rating vs Burnout Rate - Simple Bar Charts"""

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    fig.suptitle('Employee Ratings vs Burnout Rate', fontsize=16, fontweight='bold')

    # Work-Life Balance vs Burnout
    wl_data = df.groupby('rating_work_life')['high_burnout'].agg(['mean', 'count'])

    bars1 = ax1.bar(wl_data.index, wl_data['mean'],
                    color=['red', 'orange', 'yellow', 'lightgreen', 'green'], alpha=0.7)
    ax1.set_xlabel('Work-Life Balance Rating')
    ax1.set_ylabel('Burnout Rate')
    ax1.set_title('Work-Life Balance vs Burnout')
    ax1.set_ylim(0, 1)

    # Add percentage labels
    for bar, rate, count in zip(bars1, wl_data['mean'], wl_data['count']):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                f'{rate:.1%}\n(n={count})', ha='center', va='bottom', fontweight='bold')

    # Overall Rating vs Burnout
    overall_data = df.groupby('rating_overall')['high_burnout'].agg(['mean', 'count'])

    bars2 = ax2.bar(overall_data.index, overall_data['mean'],
                    color=['red', 'orange', 'yellow', 'lightgreen', 'green'], alpha=0.7)
    ax2.set_xlabel('Overall Job Rating')
    ax2.set_ylabel('Burnout Rate')
    ax2.set_title('Overall Rating vs Burnout')
    ax2.set_ylim(0, 1)

    # Add percentage labels
    for bar, rate, count in zip(bars2, overall_data['mean'], overall_data['count']):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                f'{rate:.1%}\n(n={count})', ha='center', va='bottom', fontweight='bold')

    plt.tight_layout()
    plt.show()

    # Print summary
    print("ðŸ“Š RATING INSIGHTS:")
    print(f"Low ratings (1-2): {df[df['rating_work_life'] <= 2]['high_burnout'].mean():.1%} burnout rate")
    print(f"High ratings (4-5

#!/usr/bin/env python3
"""
Ready-to-Run Burnout Visualizations
Simple code that works directly with your YouTube dataset
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

def create_burnout_visualizations(file_path='youtube_transcript_dataset.csv'):
    """
    Create 4 simple but effective burnout visualizations
    """

    print("ðŸ“Š Loading YouTube Dataset...")

    # Load data
    df = pd.read_csv(file_path)
    df = df[df['is_warehouse'] == True].copy()  # Warehouse workers only

    print(f"âœ… Loaded {len(df)} warehouse employee records")

    # Create burnout indicators
    def analyze_burnout_keywords(text):
        if pd.isna(text):
            return 0
        text_lower = str(text).lower()
        burnout_words = ['stress', 'exhausted', 'overwhelmed', 'burnout', 'tired',
                        'demanding', 'pressure', 'quit', 'hate', 'terrible']
        return sum(1 for word in burnout_words if word in text_lower)

    df['burnout_keywords_count'] = df['text'].apply(analyze_burnout_keywords)

    # Calculate burnout risk
    def calculate_burnout_risk(row):
        score = 0
        if row['rating_overall'] <= 2: score += 3
        elif row['rating_overall'] == 3: score += 1

        if row['sentiment_polarity'] < -0.3: score += 2
        elif row['sentiment_polarity'] < 0.1: score += 1

        score += min(row['burnout_keywords_count'], 3)

        if row['word_negative_count'] > row['word_positive_count']: score += 1

        if score >= 5: return 'High Risk'
        elif score >= 2: return 'Medium Risk'
        else: return 'Low Risk'

    df['burnout_risk'] = df.apply(calculate_burnout_risk, axis=1)
    df['high_burnout'] = (df['burnout_risk'] == 'High Risk').astype(int)

    print("\nBurnout Risk Distribution:")
    print(df['burnout_risk'].value_counts())

    # Create the 4 visualizations
    create_all_four_visuals(df)

    return df

def create_all_four_visuals(df):
    """Create all 4 burnout relationship visualizations"""

    # VISUAL 1: Ratings vs Burnout Rate
    print("\nðŸ“ˆ Creating Visual 1: Ratings vs Burnout Rate")

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    fig.suptitle('Employee Ratings vs Burnout Risk\nAmazon Warehouse Workers', fontsize=16, fontweight='bold')

    # Overall Rating vs Burnout
    rating_data = df.groupby('rating_overall')['high_burnout'].agg(['mean', 'count'])
    colors = ['#d32f2f', '#ff5722', '#ff9800', '#8bc34a', '#4caf50']

    bars1 = ax1.bar(rating_data.index, rating_data['mean'],
                    color=[colors[int(r)-1] for r in rating_data.index], alpha=0.8)

    ax1.set_xlabel('Overall Job Rating', fontweight='bold')
    ax1.set_ylabel('High Burnout Risk Rate', fontweight='bold')
    ax1.set_title('Job Rating vs Burnout Risk')
    ax1.set_ylim(0, 1)

    # Add labels
    for bar, rate, count in zip(bars1, rating_data['mean'], rating_data['count']):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                f'{rate:.1%}\n(n={count})', ha='center', va='bottom', fontweight='bold')

    # Sentiment vs Burnout
    df['sentiment_category'] = df['sentiment_polarity'].apply(
        lambda x: 'Negative' if x < -0.1 else ('Neutral' if x < 0.1 else 'Positive')
    )

    sent_data = df.groupby('sentiment_category')['high_burnout'].agg(['mean', 'count'])
    sent_order = ['Negative', 'Neutral', 'Positive']
    sent_colors = ['#d32f2f', '#ff9800', '#4caf50']

    bars2 = ax2.bar(range(len(sent_order)),
                    [sent_data.loc[s, 'mean'] if s in sent_data.index else 0 for s in sent_order],
                    color=sent_colors, alpha=0.8)

    ax2.set_xlabel('Sentiment Category', fontweight='bold')
    ax2.set_ylabel('High Burnout Risk Rate', fontweight='bold')
    ax2.set_title('Sentiment vs Burnout Risk')
    ax2.set_ylim(0, 1)
    ax2.set_xticks(range(len(sent_order)))
    ax2.set_xticklabels(sent_order)

    # Add labels
    for i, (bar, sentiment) in enumerate(zip(bars2, sent_order)):
        if sentiment in sent_data.index:
            rate = sent_data.loc[sentiment, 'mean']
            count = sent_data.loc[sentiment, 'count']
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                    f'{rate:.1%}\n(n={count})', ha='center', va='bottom', fontweight='bold')

    plt.tight_layout()
    plt.show()

    # VISUAL 2: Keywords vs Burnout Rate
    print("\nðŸ”‘ Creating Visual 2: Keywords vs Burnout Rate")

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    fig.suptitle('Text Analysis vs Burnout Risk\nAmazon Warehouse Workers', fontsize=16, fontweight='bold')

    # Keyword count vs burnout
    keyword_bins = [0, 1, 2, 3, 10]
    keyword_labels = ['0', '1', '2', '3+']
    df['keyword_category'] = pd.cut(df['burnout_keywords_count'], bins=keyword_bins,
                                   labels=keyword_labels, include_lowest=True)

    keyword_data = df.groupby('keyword_category')['high_burnout'].agg(['mean', 'count'])

    bars1 = ax1.bar(range(len(keyword_labels)), keyword_data['mean'],
                    color=['#4caf50', '#ff9800', '#ff5722', '#d32f2f'], alpha=0.8)

    ax1.set_xlabel('Burnout Keywords in Review', fontweight='bold')
    ax1.set_ylabel('High Burnout Risk Rate', fontweight='bold')
    ax1.set_title('Keyword Count vs Burnout Risk')
    ax1.set_ylim(0, 1)
    ax1.set_xticks(range(len(keyword_labels)))
    ax1.set_xticklabels(keyword_labels)

    # Add labels
    for bar, rate, count in zip(bars1, keyword_data['mean'], keyword_data['count']):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                f'{rate:.1%}\n(n={count})', ha='center', va='bottom', fontweight='bold')

    # Word balance vs burnout
    df['word_balance'] = df['word_positive_count'] - df['word_negative_count']
    df['balance_category'] = df['word_balance'].apply(
        lambda x: 'More Negative' if x < 0 else ('Balanced' if x == 0 else 'More Positive')
    )

    balance_data = df.groupby('balance_category')['high_burnout'].agg(['mean', 'count'])
    balance_order = ['More Negative', 'Balanced', 'More Positive']
    balance_colors = ['#d32f2f', '#ff9800', '#4caf50']

    bars2 = ax2.bar(range(len(balance_order)),
                    [balance_data.loc[cat, 'mean'] if cat in balance_data.index else 0
                     for cat in balance_order],
                    color=balance_colors, alpha=0.8)

    ax2.set_xlabel('Word Sentiment Balance', fontweight='bold')
    ax2.set_ylabel('High Burnout Risk Rate', fontweight='bold')
    ax2.set_title('Word Balance vs Burnout Risk')
    ax2.set_ylim(0, 1)
    ax2.set_xticks(range(len(balance_order)))
    ax2.set_xticklabels(balance_order, rotation=30)

    # Add labels
    for i, (bar, category) in enumerate(zip(bars2, balance_order)):
        if category in balance_data.index:
            rate = balance_data.loc[category, 'mean']
            count = balance_data.loc[category, 'count']
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                    f'{rate:.1%}\n(n={count})', ha='center', va='bottom', fontweight='bold')

    plt.tight_layout()
    plt.show()

    # VISUAL 3: Employee Characteristics vs Burnout
    print("\nðŸ‘¥ Creating Visual 3: Employee Characteristics vs Burnout")

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    fig.suptitle('Employee Characteristics vs Burnout Risk\nAmazon Warehouse Workers', fontsize=16, fontweight='bold')

    # Employee status vs burnout
    status_data = df.groupby('employee_status')['high_burnout'].agg(['mean', 'count'])

    bars1 = ax1.bar(range(len(status_data)), status_data['mean'],
                    color=['#2196f3', '#ff9800'], alpha=0.8)

    ax1.set_xlabel('Employee Status', fontweight='bold')
    ax1.set_ylabel('High Burnout Risk Rate', fontweight='bold')
    ax1.set_title('Employment Status vs Burnout')
    ax1.set_ylim(0, 1)
    ax1.set_xticks(range(len(status_data)))
    ax1.set_xticklabels(status_data.index, rotation=45)

    # Add labels
    for bar, rate, count in zip(bars1, status_data['mean'], status_data['count']):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                f'{rate:.1%}\n(n={count})', ha='center', va='bottom', fontweight='bold')

    # Review length vs burnout
    df['review_length'] = df['text'].str.len()
    length_bins = [0, 500, 1000, 2000, 10000]
    length_labels = ['Short\n(<500)', 'Medium\n(500-1k)', 'Long\n(1k-2k)', 'Very Long\n(2k+)']

    df['length_category'] = pd.cut(df['review_length'], bins=length_bins,
                                  labels=length_labels, include_lowest=True)

    length_data = df.groupby('length_category')['high_burnout'].agg(['mean', 'count'])

    bars2 = ax2.bar(range(len(length_labels)), length_data['mean'],
                    color=['#4caf50', '#ff9800', '#ff5722', '#d32f2f'], alpha=0.8)

    ax2.set_xlabel('Review Length', fontweight='bold')
    ax2.set_ylabel('High Burnout Risk Rate', fontweight='bold')
    ax2.set_title('Review Length vs Burnout Risk')
    ax2.set_ylim(0, 1)
    ax2.set_xticks(range(len(length_labels)))
    ax2.set_xticklabels(length_labels)

    # Add labels
    for bar, rate, count in zip(bars2, length_data['mean'], length_data['count']):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                f'{rate:.1%}\n(n={count})', ha='center', va='bottom', fontweight='bold')

    plt.tight_layout()
    plt.show()

    # VISUAL 4: Summary Dashboard
    print("\nðŸ“‹ Creating Visual 4: Summary Dashboard")

    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Amazon Warehouse Employee Burnout Analysis Dashboard', fontsize=18, fontweight='bold')

    # 1. Burnout distribution pie chart
    risk_dist = df['burnout_risk'].value_counts()
    colors = ['#d32f2f', '#ff9800', '#4caf50']

    wedges, texts, autotexts = ax1.pie(risk_dist.values, labels=risk_dist.index,
                                       autopct='%1.1f%%', colors=colors, startangle=90)
    ax1.set_title('Burnout Risk Distribution', fontweight='bold')

    # 2. Feature correlation with burnout
    corr_features = ['rating_overall', 'sentiment_polarity', 'burnout_keywords_count', 'word_negative_count']
    correlations = [df[feat].corr(df['high_burnout']) for feat in corr_features]

    bars = ax2.barh(range(len(corr_features)), correlations,
                    color=['red' if x < 0 else 'green' for x in correlations], alpha=0.7)

    ax2.set_yticks(range(len(corr_features)))
    ax2.set_yticklabels([f.replace('_', ' ').title() for f in corr_features])
    ax2.set_xlabel('Correlation with Burnout Risk')
    ax2.set_title('Risk Factor Correlations')
    ax2.axvline(x=0, color='black', linestyle='-', alpha=0.3)

    # Add correlation values
    for bar, corr in zip(bars, correlations):
        ax2.text(corr + (0.02 if corr > 0 else -0.02), bar.get_y() + bar.get_height()/2,
                f'{corr:.3f}', va='center', ha='left' if corr > 0 else 'right', fontweight='bold')

    # 3. Risk level comparison
    risk_comparison = ['Rating â‰¤2', 'Negative Sentiment', '2+ Keywords', 'More Neg Words']
    risk_rates = [
        df[df['rating_overall'] <= 2]['high_burnout'].mean(),
        df[df['sentiment_polarity'] < -0.1]['high_burnout'].mean(),
        df[df['burnout_keywords_count'] >= 2]['high_burnout'].mean(),
        df[df['word_negative_count'] > df['word_positive_count']]['high_burnout'].mean()
    ]

    bars3 = ax3.bar(range(len(risk_comparison)), risk_rates,
                    color=['#d32f2f', '#ff5722', '#ff9800', '#ffc107'], alpha=0.8)

    ax3.set_xlabel('Risk Indicators')
    ax3.set_ylabel('Burnout Rate')
    ax3.set_title('Burnout Rate by Risk Indicator')
    ax3.set_ylim(0, 1)
    ax3.set_xticks(range(len(risk_comparison)))
    ax3.set_xticklabels(risk_comparison, rotation=45)

    # Add percentage labels
    for bar, rate in zip(bars3, risk_rates):
        if not pd.isna(rate):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                    f'{rate:.1%}', ha='center', va='bottom', fontweight='bold')

    # 4. Key statistics
    ax4.axis('off')

    total_employees = len(df)
    high_risk_count = df['high_burnout'].sum()
    high_risk_pct = high_risk_count / total_employees * 100
    avg_rating = df['rating_overall'].mean()
    avg_sentiment = df['sentiment_polarity'].mean()

    stats_text = f"""
KEY STATISTICS

Total Employees: {total_employees}

ðŸš¨ High Burnout Risk: {high_risk_count} ({high_risk_pct:.1f}%)
âš ï¸  Medium Risk: {len(df[df['burnout_risk'] == 'Medium Risk'])}
âœ… Low Risk: {len(df[df['burnout_risk'] == 'Low Risk'])}

ðŸ“Š Average Job Rating: {avg_rating:.1f}/5
ðŸ“ˆ Average Sentiment: {avg_sentiment:.2f}

ðŸ” TOP RISK FACTORS:
â€¢ Low ratings (1-2): {df[df['rating_overall'] <= 2]['high_burnout'].mean():.1%} risk
â€¢ Negative sentiment: {df[df['sentiment_polarity'] < -0.1]['high_burnout'].mean():.1%} risk
â€¢ Multiple keywords: {df[df['burnout_keywords_count'] >= 2]['high_burnout'].mean():.1%} risk

ðŸ’¡ RECOMMENDATION:
Focus on employees with ratings â‰¤2
and negative sentiment scores
    """

    ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, fontsize=11,
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))

    plt.tight_layout()
    plt.show()

    print("\nâœ… All 4 visualizations created successfully!")

# SIMPLE ONE-LINE USAGE
def quick_burnout_analysis(file_path='youtube_transcript_dataset.csv'):
    """
    One function to run everything - just call this!
    """
    return create_burnout_visualizations(file_path)

# Example usage
if __name__ == "__main__":
    print("ðŸš€ RUNNING BURNOUT ANALYSIS")
    print("=" * 50)

    # Just run this one line:
    df = quick_burnout_analysis('youtube_transcript_dataset.csv')

    print("\nðŸ’¡ TO USE WITH YOUR DATA:")
    print("df = quick_burnout_analysis('your_file.csv')")
    print("\nOr run individual parts:")
    print("df = create_burnout_visualizations('your_file.csv')")



